{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font size=\"6\">Keras Fundamentals</font></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [Import necessary Libraries](#1)\n",
    "- [Introduction](#2)\n",
    "- [The fundamental building blocks in Keras](#3)\n",
    "    - [Layers](#3.1)\n",
    "    - [Models](#3.2)\n",
    "    - [Loss function](#3.3)\n",
    "    - [Optimizers](#3.4)\n",
    "- [Creating neural networks in Keras](#4)\n",
    "- [Conclusions](#5)\n",
    "- [References](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.16\n",
      "Numpy: 1.24.1\n",
      "Keras: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and print their versions\n",
    "from platform import python_version\n",
    "print(f\"Python: {python_version()}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "from tensorflow import keras\n",
    "print(f\"Keras: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# Introduction\n",
    "TensorFlow and Keras: TensorFlow is an open source library for neural networks and deep learning developed by the Google Brain team in november 2015. Today TensorFlow and its competitor PyTorch are the most popular machine learning libraries and are used extensively in a wide variety of real-world applications. [For example](https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3), Airbnb improves the guest experience by using TensorFlow to classify images and detect objects at scale.\n",
    "\n",
    "Keras is a high-level API based on TensorFlow. It provides a simple and intuitive API for building neural networks with TensorFlow. Its guiding principles are modularity and extensibility. This beginner-friendly approach has led to the popularity of Keras. In this notebook we will explore the basics of Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# The fundamentals building blocks in Keras\n",
    "One of the basic building blocks in Keras are layers. We can stack these linearly to build a model. Another building block is the loss function we choose, which provides the metrics we will use to train our model using an optimizer. With these basic structures, we can build any neural network.\n",
    "\n",
    "The following diagram illustrates the relationship between these building blocks in Keras:\n",
    "\n",
    "![Building Blocks in Keras](pictures/building_blocks_keras.png)\n",
    "\n",
    "<a id='3.2'></a>\n",
    "## Layers\n",
    "Layers are the smallest unit of our neural network. Each layer takes an input, performs a mathematical function and then outputs it to the next layer. The most important layers in Keras include dense layers, activation layers and dropout layers. However, there are also other, more complex layers such as convolutional layers and pooling layers.\n",
    "\n",
    "Let us first take a closer look at the dense layer. This type of layer is by far the most common type of layer used in Keras. A dense layer is also known as a fully connected layer because it uses all of its input for the mathematical function it implements. **Dense** implements the operation: **output = activation(dot(input,kernel) + bias)** where **activation** is the element-wise activation function passed as the **activation** argument, **kernel** is a weight matrix created by the layer, and **bias** is a bias vector created by the layer (only applicable if **use_bias** is **True**). These are all attributes of **Dense**.\n",
    "\n",
    "<a id='3.2'></a>\n",
    "## Models\n",
    "A model is a collection of layers, and the most commonly used model in Keras is the **Sequential** model. A **Sequential** model allows us to stack layers linearly on top of each other, where a single layer is only connected to another layer. In this way, we can easily design model architectures without worrying about the underlying mathematics. This is very practical, because you have to think a lot about making sure that the successive layer dimensions are compatible with each other.\n",
    "\n",
    "Once we have defined our model architecture, we need to define our training process, which is done with the **.compile** statement. The **compile** method takes several arguments, but the most important arguments we need to define are the **optimizer** and the **loss** function.\n",
    "\n",
    "<a id='3.3'></a>\n",
    "## Loss function\n",
    "There are several loss functions implemented in Keras, but the most commonly used loss functions are: **mean_squared_error**, **categorical_crossentropy**, and **binary_crossentropy**.\n",
    "As a general rule, this is how you should choose which loss function to use:\n",
    "- **mean_squared_error** if the problem is a regression problem\n",
    "- **categorical_crossentropy** if the problem is a multiclass classification problem\n",
    "- **binary_crossentropy** if the problem is a binary classification problem\n",
    "\n",
    "In certain cases, you might find that the default loss functions in Keras are unsuitable for your problem. In this case, you can define your own loss function by defining a custom function in Python and then passing it to the **compile** statement in Keras.\n",
    "\n",
    "<a id='3.4'></a>\n",
    "## Optimizers\n",
    "An optimizer is an algorithm for updating the weights of the neural network in the training process to minimize the loss function. The loss function acts as a guide telling the optimizer if its moving in the right direction to reach the global minimum.\n",
    "\n",
    "In this notebook we will only discuss some of the most popular optimizers in Keras. Others can be found [here](https://keras.io/api/optimizers/).\n",
    "- SGD: Stochastic gradient descent (SGD) is the standard optimiser for Keras. SGD differs from regular gradient descent in the way it calculates the gradient. Instead of using all the training data to calculate the gradient per epoch, a randomly selected instance from the training data is used to estimate the gradient. This generally leads to faster convergence, but the steps are noisier because each step is an estimate. In general, researchers have found that the **sgd** optimiser works best for flat neural networks.\n",
    "- Adagrad: Adagrad is the abbreviation for adaptive gradients. Essentially, what happens here is this: If a weight has experienced very large updates, its cache value will also increase. As a result, the learning rate will decrease and the update sizes of this weight will decrease over time. On the other hand, if a weight has not experienced significant updates, its cache value will be very low and therefore its learning rate will increase, forcing it to make larger updates. This is the basic principle of the **adagrad** optimiser. However, the disadvantage of this algorithm is that regardless of the past gradients of a weight, the cache will always increase by a certain amount because the square cannot be negative. Therefore, the learning rate of each weight will eventually decrease to a very low value until training no longer occurs significantly.\n",
    "- RMSProp: Root Mean Squared Propagation (RMSProp) solves the problem of the previous optimiser by introducing a new parameter, the decay rate. This ensures that the learning rate is constantly changing depending on how the weighting is updated, just like **adagrad**, but at the same time the learning rate does not decrease too quickly so that training can continue for much longer.\n",
    "- Adam: Adaptive Moment Estimation (Adam) is widely regarded as one of the best optimisers for Deep Learning in general. **Adam** is a combination of **RMSProp** and Momentum. Here we perform gradient accumulation by computing Momentum, and we also constantly change the learning rate by using Cache. Because of these two properties, **Adam** usually performs better than all other optimisers and is usually preferred when training a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## Creating neural networks in Keras\n",
    "Let's look at how we can create a neural network with two layers in Keras. To create a linear collection of layers, first declare a **sequential** model in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an empty **Sequential** model to which we can now add layers. We start by adding layers from the left (the layer closest to the input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "model.add(keras.layers.Dense(units=4, activation=\"sigmoid\", input_dim=3))\n",
    "# Layer 2 (Output Layer)\n",
    "model.add(keras.layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the instruction **model.add()** in Keras we can stack layers on top of each other. At the beginning we had to define the number of units in each layer. In general, a higher number of units increases the complexity of the model, as this means that more weights have to be trained. We also had to define the input for the first layer with **input_dim**. This informs Keras about the number of features (number of columns in our dataset) that our data has.\n",
    "\n",
    "![Two-Layer Neural Network](pictures/neural_network.png)\n",
    "\n",
    "We can check the structure of our model by calling the function **model.summary()** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 16        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters is the number of weights and biases we need to train for the model we just defined.\n",
    "\n",
    "Once we have finished building the architecture of our model, we can compile it and start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(learning_rate=1)\n",
    "model.compile(optimizer=sgd, loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We have set the learning rate of the **sgd** optimiser to 1.0. In general, the learning rate is a hyperparameter of the neutral network that needs to be set carefully depending on the problem.\n",
    "\n",
    "Let us now create some data with which we can train our neural network. Define an **X** and **y** numpy array corresponding to the features and the target variables respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Feature variables\n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "# Target variables\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bf9124df70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train our model for 1500 iterations (epochs)\n",
    "model.fit(X, y, epochs=1500, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0567945 ],\n",
       "       [0.94023657],\n",
       "       [0.94039214],\n",
       "       [0.05710956]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the predictions\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We have successfully created and trained a neural network with Keras. We can verify this with our predictions, which converge with the true values.\n",
    "\n",
    "Note: There is a slight difference between the predictions and the true values. This is desirable as it prevents overfitting and allows the neural network to generalise better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# Conclusions\n",
    "In this notebook we have programmed our own little neural network with Keras and got a feel for how Keras and its building blocks work. We also took a look at different optimisers and learned what makes them stand out. From now on, we can take our deep learning projects to the next level and solve real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "# References\n",
    "1. [Numpy](https://numpy.org/doc/1.21/)\n",
    "2. [Keras](https://keras.io/api/)\n",
    "3. [Diagrams](https://www.diagrams.net/)\n",
    "4. [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)\n",
    "5. [Understanding Optimizers](https://medium.com/deep-learning-demystified/https-medium-com-deep-learning-demystified-understanding-optimizers-313b787a69fe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
